{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once data has been loaded and some initial cleaning is done, it is often necessary to perform several addition rounds of data preparation.  This can include scaling data, removing outliers, re-encoding variables, and imputing missing data.  We'll cover these aspects below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Data\n",
    "\n",
    "Many machine learning algorithms, such as k-means clustering and support vector machines, use distance metrics like Euclidean distance to compare points in the feature space. Features with large numeric ranges can dominate the distance computation, thereby affecting the algorithm's performance. Similarly, optimization algorithms like gradient descent converge more quickly when features are on similar scales.\n",
    "\n",
    "### 2. Working Example - Impact of Scaling on K-Means Clustering\n",
    "\n",
    "Let's consider a simple synthetic dataset with two features `X1` and `X2`, where `X1` has values ranging between 0 and 10, but are separated into two groups along the axis. `X2` ranges from 0 to 1000, and so dominates the distance function. We'll cluster the data using k-means before and after scaling to see the difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X1a = np.random.uniform(0, 4, 50)\n",
    "X1b = np.random.uniform(6, 10, 50)\n",
    "X1 = np.concatenate([X1a,X1b])\n",
    "X2 = np.random.uniform(0, 1000, 100)\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Cluster without scaling\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.title('K-Means Clustering without Scaling')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.show()\n",
    "\n",
    "# Scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Cluster after scaling\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='viridis')\n",
    "plt.title('K-Means Clustering with Scaling')\n",
    "plt.xlabel('X1 (scaled)')\n",
    "plt.ylabel('X2 (scaled)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. How to scale\n",
    "\n",
    "Not all distributions are created equal!  It is important to examine your distributions before scaling parameters, or else your scaling efforts might not yield any improvements.\n",
    "\n",
    "Two common types of distributions are normal and power-law (heavy-tailed) distributions.  These are easy to recognize by plotting histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "n_samples = 1000\n",
    "\n",
    "# Power-law (exponential) distribution\n",
    "power_law_data = np.random.exponential(scale=10, size=n_samples)\n",
    "\n",
    "# Scaled\n",
    "scaled_power_law = np.log1p(power_law_data)\n",
    "\n",
    "# Normal distribution\n",
    "normal_data = np.random.normal(loc=50, scale=10, size=n_samples)\n",
    "normal_data_2D = normal_data.reshape(-1, 1)\n",
    "\n",
    "# Apply scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_normal_data_2D = scaler.fit_transform(normal_data_2D)\n",
    "\n",
    "# Convert back to 1D array\n",
    "scaled_normal_data = scaled_normal_data_2D.ravel()\n",
    "\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot power-law distribution\n",
    "axes[0,0].hist(power_law_data, bins=30, color='blue', edgecolor='black')\n",
    "axes[0,0].set_title(\"Power-law (Exponential) Distribution\")\n",
    "axes[0,0].set_xlabel(\"Value\")\n",
    "axes[0,0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Plot scaled power-law distribution\n",
    "axes[1,0].hist(scaled_power_law, bins=30, color='blue', edgecolor='black')\n",
    "axes[1,0].set_title(\"Scaled Exponential Distribution\")\n",
    "axes[1,0].set_xlabel(\"Value\")\n",
    "axes[1,0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Plot normal distribution\n",
    "axes[0,1].hist(normal_data, bins=30, color='green', edgecolor='black')\n",
    "axes[0,1].set_title(\"Normal Distribution\")\n",
    "axes[0,1].set_xlabel(\"Value\")\n",
    "axes[0,1].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Plot scaled normal distribution\n",
    "axes[1,1].hist(scaled_normal_data, bins=30, color='green', edgecolor='black')\n",
    "axes[1,1].set_title(\"Normal Distribution\")\n",
    "axes[1,1].set_xlabel(\"Value\")\n",
    "axes[1,1].set_ylabel(\"Frequency\")\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- **Z-scaling**: Use it when the feature roughly follows a normal distribution or when you don't have information about the distribution. It transforms the data into a distribution with a mean of 0 and a standard deviation of 1.\n",
    "  \n",
    "- **Log-Scaling**: It is useful for features that follow a power-law distribution. In these cases, log-scaling can help equalize the ranges and variances across features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In the following, I've created a sample dataset with an exponential feature and a normal feature. Try using the different scaling methods before running the classifier.  How do your results change:\n",
    "\n",
    "1.  If you scale the exponential feature using a StandardScaler\n",
    "2.  If you scale the exponential feature using a Log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create synthetic dataset\n",
    "n_samples = 1000\n",
    "\n",
    "# Feature 1: Power law (exponential) distribution\n",
    "X1 = np.random.exponential(scale=5, size=n_samples)\n",
    "\n",
    "# Feature 2: Normal distribution\n",
    "X2 = np.random.normal(loc=50, scale=10, size=n_samples)\n",
    "\n",
    "# Create labels: simple linear relation to X1 and X2\n",
    "y = np.array([1 if x1 + 0.001 * x2 > 1 else 0 for x1, x2 in zip(X1, X2)])\n",
    "flip_indices = np.random.choice(n_samples, size=int(0.1 * n_samples), replace=False)\n",
    "y[flip_indices] = 1 - y[flip_indices]\n",
    "\n",
    "\n",
    "\n",
    "# Combine features into single data array\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression without scaling\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "print(f\"Logistic Regression without Scaling - Test Accuracy: {lr.score(X_test, y_test):.2f}\")\n",
    "\n",
    "# Plot original features\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel('Feature 1 (Power law)')\n",
    "plt.ylabel('Feature 2 (Normal)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data\n",
    "\n",
    "When dealing with missing data, understanding the mechanism behind the missingness can guide the selection of appropriate imputation methods. The three primary types of missing data are:\n",
    "\n",
    "1. **Missing Completely At Random (MCAR)**: The missingness has no relationship with any other variable, observed or unobserved. It's random.\n",
    "\n",
    "   - **Preferred Imputation Methods**: Mean, median, or mode imputation can be considered. More sophisticated techniques like k-Nearest Neighbors (k-NN) imputation can also be used.\n",
    "   \n",
    "   - **Notes**: Because the data is missing randomly, simpler methods often perform adequately.\n",
    "\n",
    "   - **Example**: Suppose you're collecting data on the heights and weights of a class of students. If you lost some pages of your data collection notebook and that loss is unrelated to the actual data points themselves, then the missing data can be considered MCAR.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Synthetic data for height and weight of students\n",
    "np.random.seed(0)\n",
    "n_students = 100\n",
    "height = np.random.normal(160, 10, n_students)  # Average height is 160 cm with a standard deviation of 10\n",
    "\n",
    "weight = 40 + height / 8 # simulate correlation between height and weight\n",
    "noise = np.random.normal(0, 10, n_students)\n",
    "weight+=noise   \n",
    "\n",
    "# Randomly introduce missing data (MCAR)\n",
    "missing_ratio = 0.2  # 20% of the data will be missing\n",
    "missing_indices = np.random.choice(n_students, int(n_students * missing_ratio), replace=False)\n",
    "\n",
    "# Make copies of original data for manipulation\n",
    "height_missing = height.copy()\n",
    "weight_missing = weight.copy()\n",
    "\n",
    "# Introduce missing data\n",
    "height_missing[missing_indices] = np.nan\n",
    "weight_missing[missing_indices] = np.nan\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Scatter plot of original data\n",
    "axes[0].scatter(height, weight, c='blue', label='Original Data')\n",
    "axes[0].set_title('Original Data')\n",
    "axes[0].set_xlabel('Height (cm)')\n",
    "axes[0].set_ylabel('Weight (kg)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Scatter plot after introducing missing data\n",
    "axes[1].scatter(height_missing, weight_missing, c='orange', label='Data with Missing Values (MCAR)')\n",
    "axes[1].scatter(height[missing_indices], weight[missing_indices], c='blue', label='Omitted Data Points', alpha=0.2)\n",
    "axes[1].set_title('Data with Missing Values (MCAR)')\n",
    "axes[1].set_xlabel('Height (cm)')\n",
    "axes[1].set_ylabel('Weight (kg)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Missing At Random (MAR)**: The probability of missingness may be related to observed data but not the missing data.\n",
    "\n",
    "   - **Preferred Imputation Methods**: Regression imputation, multiple imputation, and expectation-maximization are often used for MAR data.\n",
    "   \n",
    "   - **Notes**: These methods make use of other observed variables to predict and impute the missing variable.\n",
    "\n",
    "   - **Example**: Suppose you're conducting a survey on income and age. Younger people might be less likely to disclose their income, but within the \"young people\" group, the chance of income being missing is random.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data for income and age\n",
    "np.random.seed(0)\n",
    "n_people = 100\n",
    "age = np.random.uniform(20, 60, n_people)  # Age ranges from 20 to 60\n",
    "income = 30000 + age * 700  # Simulate a positive correlation between age and income\n",
    "\n",
    "# Add some noise to the income\n",
    "noise = np.random.normal(0, 5000, n_people)\n",
    "income += noise\n",
    "\n",
    "# Introduce missing data (MAR) for younger people\n",
    "# Assume people aged less than 40 are less likely to disclose income\n",
    "missing_ratio_young = 0.4  # 40% of young people's income will be missing\n",
    "young_indices = np.where(age < 40)[0]\n",
    "missing_indices_young = np.random.choice(young_indices, int(len(young_indices) * missing_ratio_young), replace=False)\n",
    "\n",
    "# Make copies of original data for manipulation\n",
    "income_missing = income.copy()\n",
    "\n",
    "# Introduce missing data\n",
    "income_missing[missing_indices_young] = np.nan\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Scatter plot of original data\n",
    "axes[0].scatter(age, income, c='blue', label='Original Data')\n",
    "axes[0].set_title('Original Data')\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[0].set_ylabel('Income ($)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Scatter plot after introducing missing data\n",
    "axes[1].scatter(age, income_missing, c='orange', label='Data with Missing Values (MAR)')\n",
    "axes[1].scatter(age[missing_indices_young], income[missing_indices_young], c='blue', label='Omitted Data Points', alpha=0.2)\n",
    "axes[1].set_title('Data with Missing Values (MAR)')\n",
    "axes[1].set_xlabel('Age')\n",
    "axes[1].set_ylabel('Income ($)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "3. **Missing Not At Random (MNAR)**: The probability of missingness is related to the missing data itself or a combination of missing and observed data.\n",
    "\n",
    "   - **Preferred Imputation Methods**: More complex methods such as multiple imputation using chained equations or advanced model-based methods can be used. In some cases, sensitivity analyses are performed.\n",
    "  \n",
    "   - **Notes**: Handling MNAR is particularly challenging because you have to model the missingness mechanism itself. This often involves making unverifiable assumptions.\n",
    "\n",
    "   - **Example**: In a medical study, patients with severe symptoms are less likely to return for follow-up. Here, the missingness of the data (follow-up results) is directly related to the outcome (severity of symptoms).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data for symptom severity and follow-up results\n",
    "np.random.seed(0)\n",
    "n_patients = 100\n",
    "severity = np.random.uniform(1, 10, n_patients)  # Severity ranges from 1 to 10\n",
    "follow_up = 30 - severity * 1.5  # Simulate a negative correlation between severity and follow-up results\n",
    "\n",
    "# Add some noise to the follow-up results\n",
    "noise = np.random.normal(0, 3, n_patients)\n",
    "follow_up += noise\n",
    "\n",
    "# Introduce missing data (MNAR) for patients with severe symptoms\n",
    "# Assume patients with severity > 7 are less likely to have follow-up\n",
    "missing_ratio_severe = 0.6  # 60% of severe patients' follow-up will be missing\n",
    "severe_indices = np.where(severity > 7)[0]\n",
    "missing_indices_severe = np.random.choice(severe_indices, int(len(severe_indices) * missing_ratio_severe), replace=False)\n",
    "\n",
    "# Make copies of original data for manipulation\n",
    "follow_up_missing = follow_up.copy()\n",
    "\n",
    "# Introduce missing data\n",
    "follow_up_missing[missing_indices_severe] = np.nan\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Scatter plot of original data\n",
    "axes[0].scatter(severity, follow_up, c='green', label='Original Data')\n",
    "axes[0].set_title('Original Data')\n",
    "axes[0].set_xlabel('Symptom Severity')\n",
    "axes[0].set_ylabel('Follow-Up Results')\n",
    "axes[0].legend()\n",
    "\n",
    "# Scatter plot after introducing missing data\n",
    "axes[1].scatter(severity, follow_up_missing, c='purple', label='Data with Missing Values (MNAR)')\n",
    "axes[1].scatter(severity[missing_indices_severe], follow_up[missing_indices_severe], c='green', label='Omitted Data Points', alpha=0.1)\n",
    "axes[1].set_title('Data with Missing Values (MNAR)')\n",
    "axes[1].set_xlabel('Symptom Severity')\n",
    "axes[1].set_ylabel('Follow-Up Results')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Imputation Methods:\n",
    "\n",
    "1. **Simple Imputation**: Mean, median, or mode imputation is quick and easy but can reduce the variance of the imputed variables and underestimate errors.  Note that this method only uses information from the column in which data is missing.\n",
    "\n",
    "2. **k-Nearest Neighbors (k-NN) Imputation**: This is a more advanced form of imputation that can be used for MCAR or MAR types of missingness. **Important** The KNN approach is based on the \"K Nearest Neighbors\" ML approach, which we'll talk about later.  However, it's important to know that it uses information from other columns in order to identify the most similar rows, and then derives the missing value from that.\n",
    "\n",
    "3. **Multiple Imputation**: This technique is more robust and accounts for the uncertainty of missing values. It is often used for MAR and MNAR types of missingness.\n",
    "\n",
    "4. **Model-based Imputation**: Methods like regression imputation or using machine learning models to predict missing values can be considered, especially when the data is MAR or MNAR.\n",
    "\n",
    "5. **Sensitivity Analysis**: When you have MNAR data, it's often good to perform a sensitivity analysis to understand how your results might change under different assumptions about the missingness mechanism.\n",
    "\n",
    "Would you like to include any of these points in your slides, or would you like to delve into one of these topics more deeply?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Imputation by Chained Equations (MICE)\n",
    "\n",
    "Multiple Imputation by Chained Equations (MICE), also known as Fully Conditional Specification (FCS), is a sophisticated method to handle missing data. It is especially useful when you have multiple variables with missing values and those missing values may be Missing At Random (MAR) or even Missing Not At Random (MNAR) to some extent.\n",
    "\n",
    "#### How MICE Works:\n",
    "\n",
    "1. **Initialization**: Start by filling in the missing values with a simple imputation method like mean imputation.\n",
    "  \n",
    "2. **Iterative Imputation**: \n",
    "    - For each variable with missing data:\n",
    "        1. Set its missing values back to missing.\n",
    "        2. Model this variable using the other variables.\n",
    "        3. Use this model to impute the missing values.\n",
    "    - Repeat this process for several iterations to make the imputations more robust.\n",
    "\n",
    "3. **Multiple Datasets**: This process is repeated to create multiple datasets, and analyses are performed on each. \n",
    "\n",
    "4. **Pooling**: Results from these multiple analyses are pooled together to get final estimates.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "\n",
    "<img src=\"assets/MICE.png\" width=\"500\"/>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Key Features:\n",
    "\n",
    "- **Chained Equations**: Each variable with missing data gets its own imputation model. This allows for different types of variables (e.g., continuous, ordinal, nominal).\n",
    "  \n",
    "- **Uncertainty**: By creating multiple datasets, MICE captures the uncertainty of missing values, thus providing a more accurate estimate of standard errors.\n",
    "\n",
    "- **Flexibility**: You can specify different imputation models for different variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Imputation Methods\n",
    "\n",
    "It's informative to examine the impact of different imputation methods on distributions of data.  Note that this does not necessarily correlate with the performance of an ML algorithm!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Create synthetic dataset with specific distributions\n",
    "np.random.seed(0)\n",
    "n = 200  # Number of samples\n",
    "X1 = np.random.normal(5, 2, n)  # Normal distribution\n",
    "X2 = np.random.exponential(1, n)  # Exponential distribution\n",
    "X3 = np.random.uniform(0, 10, n)  # Uniform distribution\n",
    "\n",
    "X_complete = np.column_stack((X1, X2, X3))\n",
    "\n",
    "# Introduce some missing values\n",
    "X_missing = np.copy(X_complete)\n",
    "X_missing[np.random.randint(0, n, 20), 0] = np.nan\n",
    "X_missing[np.random.randint(0, n, 20), 1] = np.nan\n",
    "X_missing[np.random.randint(0, n, 20), 2] = np.nan\n",
    "\n",
    "# Mean Imputation as an initial step\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "X_initial_imputed = imp.fit_transform(X_missing)\n",
    "\n",
    "# MICE Imputation\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "X_mice_imputed = imp.fit_transform(X_missing)\n",
    "\n",
    "# KNN Imputation\n",
    "imp = KNNImputer()\n",
    "X_knn_imputed = imp.fit_transform(X_missing)\n",
    "\n",
    "# Random Forest Imputation\n",
    "# Note - there is no built in method for RandomForest imputation, so we manually use a \n",
    "# RandomForestRegressor here to predict missing elements based on other elements.\n",
    "X_rf_imputed = X_initial_imputed.copy()\n",
    "for i in range(X_missing.shape[1]):\n",
    "    missing_idx = np.isnan(X_missing[:, i])\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    rf.fit(X_initial_imputed[~missing_idx, :][:, np.arange(X_missing.shape[1]) != i], X_initial_imputed[~missing_idx, i])\n",
    "    X_rf_imputed[missing_idx, i] = rf.predict(X_initial_imputed[missing_idx, :][:, np.arange(X_missing.shape[1]) != i])\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(6, 3, figsize=(20, 20))\n",
    "\n",
    "# Titles\n",
    "titles = ['Normal Feature', 'Exponential Feature', 'Uniform Feature']\n",
    "imputation_methods = ['Original Complete', 'With Missing Values', 'Mean Imputed', 'MICE Imputed','Random Forest Imputed', 'KNN Imputed']\n",
    "\n",
    "# Loop to plot histograms\n",
    "for j, dataset in enumerate([X_complete, X_missing, X_initial_imputed, X_mean_imputed, X_rf_imputed, X_knn_imputed]):\n",
    "    for i in range(3):\n",
    "        axes[j, i].hist(dataset[:, i][~np.isnan(dataset[:, i])], bins=20, edgecolor='black')\n",
    "        axes[j, i].set_title(f\"{titles[i]} - {imputation_methods[j]}\")\n",
    "        axes[j, i].set_xlabel('Value')\n",
    "        axes[j, i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Making scikit learn and pandas work together\n",
    "\n",
    "The interaction between pandas and scikit-learn can be a bit tricky due to the difference in data structures they expect. Let's take an example using `SimpleImputer` and a single column in a pandas DataFrame to highlight some of these issues.\n",
    "\n",
    "#### What NOT to Do\n",
    "\n",
    "One common mistake is to directly use pandas Series without reshaping it, resulting in an array with a single dimension. This can sometimes cause issues with scikit-learn estimators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create a DataFrame with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4, 5]})\n",
    "\n",
    "# Initialize SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# This would NOT work as expected, raises an error or gives unwanted results\n",
    "df['A'] = imputer.fit_transform(df['A'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The issue arises because `df['A']` is a pandas Series, and passing it to `fit_transform` without reshaping might not work as expected.\n",
    "\n",
    "#### What to Do\n",
    "\n",
    "The correct approach is to ensure that you are working with a 2D array-like structure, which is what scikit-learn's `SimpleImputer` expects. One way to do this is to use double square brackets when slicing the DataFrame to keep it as a DataFrame with one column.\n",
    "\n",
    "Here's how to do it correctly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would work as expected, returns a DataFrame\n",
    "correct_df = df[['A']].copy()\n",
    "\n",
    "# Impute missing values\n",
    "correct_df_imputed = imputer.fit_transform(correct_df)\n",
    "\n",
    "# Replace the original column with the imputed one\n",
    "df['A'] = correct_df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Or, you could also reshape the Series to make it into a 2D array explicitly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would work as well, returns a 2D numpy array\n",
    "correct_series = df['A'].values.reshape(-1, 1)\n",
    "\n",
    "# Impute missing values\n",
    "correct_series_imputed = imputer.fit_transform(correct_series)\n",
    "\n",
    "# Replace the original column with the imputed one\n",
    "df['A'] = correct_series_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In both of these correct approaches, we're ensuring that the data passed to `SimpleImputer` is a 2D array-like structure, thus meeting scikit-learn's expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "The **Titanic dataset** contains the passenger list for all 768 passengers on the Titanic, as well as who survived.  Note that some features may or may not be meaningful for machine learning purposes.\n",
    "\n",
    "The following columns are available in the original dataset:\n",
    "\n",
    "1. **PassengerId**: An unique identifier for each passenger.\n",
    "2. **Survived**: Whether the passenger survived or not. 1 for survived, 0 for deceased.\n",
    "3. **Pclass**: The ticket class, indicating the socio-economic status of the passenger (1st = Upper, 2nd = Middle, 3rd = Lower).\n",
    "4. **Name**: The full name of the passenger, sometimes including titles.\n",
    "5. **Sex**: Gender of the passenger, usually 'male' or 'female'.\n",
    "6. **Age**: Age of the passenger in years, although it's fractional if the age is less than one.\n",
    "7. **SibSp**: The number of siblings or spouses aboard.\n",
    "8. **Parch**: The number of parents or children aboard.\n",
    "9. **Ticket**: The ticket number.\n",
    "10. **Fare**: The amount of money paid for the ticket.\n",
    "11. **Cabin**: The cabin number where the passenger stayed. This field contains many missing values.\n",
    "12. **Embarked**: Port of embarkation, where C = Cherbourg, Q = Queenstown, S = Southampton.\n",
    "\n",
    "Load the titanic data set from the seaborn library. Note that some columns are redundant, or may be labeled somewhat differently than in the original, and several are missing data.  Your task:\n",
    "\n",
    "1. Load the dataset and identify the columns that have nulls.  Which columns do you think you should try to fill in?  Why?\n",
    "2. For those that seem like they might matter for machine learning, apply the following methods to fill in nulls:\n",
    "    - Fill forward, using 'ffill' (see Monday's lecture)\n",
    "    - Use a \"mean\" with a SimpleImputer\n",
    "    - Use a \"most_frequent\" with a simple imputer\n",
    "    - Try to use a KNN imputer (see the important note above!).  It won't work!  What does the error say?\n",
    "3. Compare histograms for your imputation methods.  Which would you pick?  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "titanic_data = sns.load_dataset('titanic')\n",
    "titanic_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
